{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import platform\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import matplotlib.pylab as plt\n",
    "import spikeinterface.full as si    # may need to run pip install in the spikeinterface folder first\n",
    "import probeinterface as pi\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# import ipywidgets\n",
    "%matplotlib widget\n",
    "\n",
    "# Import my custom modules\n",
    "from src import spikeinterface_hp \n",
    "from src import load_probe_rhd \n",
    "import importlib  # Allow module reloads in case any changes were made after starting the kernel: importlib.reload(spikeinterface_hp)\n",
    "\n",
    "# Add path to kilosort2 and ironclust repos\n",
    "si.Kilosort2Sorter.set_kilosort2_path(Path.resolve(Path('../../Kilosort-2.0')))\n",
    "si.IronClustSorter.set_ironclust_path(Path.resolve(Path('../../ironclust')))\n",
    "\n",
    "# Print SI version\n",
    "print(f\"SpikeInterface version: {si.__version__}\")\n",
    "\n",
    "# Print list of installed sorters\n",
    "# print(si.installed_sorters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike sort with Ironclust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: modify params\n",
    "# default_ic_params = si.IronClustSorter.default_params()\n",
    "# pprint(default_ic_params)\n",
    "# si.get_params_description('ironclust')\n",
    "\n",
    "# Spike sort all files in spreadsheet with Ironclus\n",
    "T = pd.read_excel(Path('D:/hannah/Dropbox/alab/Analysis/RECORDING_DEPTH_CHICK.xlsx'), sheet_name=0, header=0)\n",
    "T = T.drop(T[T.exclude==1].index)\n",
    "T_height = T.shape\n",
    "T_height = T_height[0]\n",
    "T_filename = T[\"filename\"]\n",
    "T_bad_chan = T[\"bad_chan\"]\n",
    "\n",
    "pprint(T)\n",
    "\n",
    "overwrite = True\n",
    "\n",
    "# for ii in range(21,T_height):\n",
    "for ii in range(T_height):\n",
    "    try:\n",
    "\n",
    "        # Load the raw ephys recording\n",
    "        results_path = Path('Z:/Hannah/ephys/project2/')/T_filename.iloc[ii]   #  A 32 channel recording, shank A only\n",
    "        data_path = [f for f in results_path.iterdir() if f.is_dir() and f.name.startswith('raw')] # subfolder starting with raw*. Should only be one!\n",
    "        data_path = data_path[0] # Set data_path to the folder containing the intan.rhd file etc\n",
    "        print(data_path)\n",
    "        recording_raw = spikeinterface_hp.read_intan_dat(data_path)\n",
    "        # Load the probe for this file - should automatically deal with missing channels!\n",
    "        probe = load_probe_rhd.H6(data_path/'info.rhd', results_path/'probe.json')\n",
    "        recording_raw = recording_raw.set_probe(probe, group_mode=\"by_shank\")\n",
    "\n",
    "        # TODO: remove bad_chan (debugging below)\n",
    "        # channnel_ids = recording_raw.channel_ids\n",
    "        # good_ids = []\n",
    "        # bad_ids = T_bad_chan.iloc[ii]\n",
    "        # good_ids = [recording_raw.channel_ids[i] for i in range(recording_raw.get_num_channels()) if i not in bad_ids]\n",
    "        # recording_raw = recording_raw.channel_slice(good_ids)\n",
    "        # probe2 = recording_raw.get_probe()\n",
    "        # si.plot_probe(probe2)\n",
    "\n",
    "        # Run sorter if not already run\n",
    "        sorter_name = 'ironclust'\n",
    "        sorting_folder = results_path/(sorter_name+'_output')\n",
    "        sorting_folder_si = results_path/(sorter_name+'_si_output')\n",
    "        print(sorting_folder_si)\n",
    "\n",
    "        # TEMP\n",
    "        if sorting_folder.is_dir():\n",
    "            print('Overwriting')\n",
    "            shutil.rmtree(sorting_folder, ignore_errors=True)\n",
    "\n",
    "\n",
    "        if sorting_folder_si.is_dir():\n",
    "            if overwrite:\n",
    "                print('Overwriting')\n",
    "                shutil.rmtree(sorting_folder_si, ignore_errors=True)\n",
    "            else:\n",
    "                print('skipped, already run')\n",
    "                continue\n",
    "        sorting_ic = si.run_sorter(sorter_name, recording_raw, sorting_folder, verbose=True, remove_existing_folder=True) # filter=False, detect_threshold=1\n",
    "        print(sorting_ic)\n",
    "\n",
    "        # Save sorting in spike interface .npz format\n",
    "        sorting_ic.save(folder=sorting_folder_si)\n",
    "\n",
    "        # Delete the useless & large original ironclust output folder\n",
    "        shutil.rmtree(sorting_folder, ignore_errors=True)\n",
    "\n",
    "    except Exception:\n",
    "        print('skipped, error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spike sort all files in spreadsheet with Ironclus\n",
    "T = pd.read_excel(Path('D:/hannah/Dropbox/alab/Analysis/RECORDING_DEPTH_CHICK.xlsx'), sheet_name=0, header=0)\n",
    "T = T.drop(T[T.exclude==1].index)\n",
    "T_height = T.shape\n",
    "T_height = T_height[0]\n",
    "T_filename = T[\"filename\"]\n",
    "T_bad_chan = T[\"bad_chan\"]\n",
    "\n",
    "recording_raw = spikeinterface_hp.read_intan_dat(data_path)\n",
    "probe = load_probe_rhd.H6(data_path/'info.rhd', results_path/'probe.json')\n",
    "recording_raw = recording_raw.set_probe(probe, group_mode=\"by_shank\")\n",
    "\n",
    "channnel_ids = recording_raw.channel_ids\n",
    "pprint(channnel_ids)\n",
    "\n",
    "bad_ids = np.array(eval(T_bad_chan.iloc[ii]))-1\n",
    "# pprint(bad_ids)\n",
    "good_ids = [recording_raw.channel_ids[i] for i in range(recording_raw.get_num_channels()) if i not in bad_ids]\n",
    "# pprint(good_ids)\n",
    "# recording_raw = recording_raw.channel_slice(range(32))\n",
    "\n",
    "probe = recording_raw.get_probe()\n",
    "# pi.plotting.plot_probe(probe,with_contact_id=True)\n",
    "si.plot_probe_map(recording_raw, with_channel_index=True) #with_device_index #with_channel_index with_contact_id with_channel_ids\n",
    "# si.plot_probe_map(recording, with_channel_ids=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find consensus units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii in range(T_height):\n",
    "ii = 21\n",
    "\n",
    "# Load the raw ephys recording\n",
    "results_path = Path('Z:/Hannah/ephys/project2/')/T_filename.iloc[ii]   #  A 32 channel recording, shank A only\n",
    "print(results_path)\n",
    "\n",
    "# Load two sortings\n",
    "sorting_ks = si.read_kilosort(results_path/'kilosort2_output')\n",
    "sorting_ic = si.load_extractor(results_path/'ironclust_si_output') # Load the sorting results (only needed if picking up from here)\n",
    "print(sorting_ks)\n",
    "print(sorting_ic)\n",
    "\n",
    "# Compare two sorters\n",
    "comp_ks_ic = si.compare_two_sorters(sorting1=sorting_ks, sorting2=sorting_ic, match_score=.2)  # returns SortingComparison object\n",
    "match_to_ks = comp_ks_ic.hungarian_match_12\n",
    "print('Relative to Kilosort2:')\n",
    "pprint(match_to_ks)\n",
    "\n",
    "# Plot agreement matrix *** comment if looping\n",
    "si.plot_agreement_matrix(comp_ks_ic)\n",
    "\n",
    "# Select only the matched units from the kilosort sorting\n",
    "ks_ind = np.nonzero([match_to_ks!=-1])[1]\n",
    "print(ks_ind)\n",
    "sorting_clean = sorting_ks.select_units(ks_ind)\n",
    "print(sorting_clean)\n",
    "\n",
    "# Export the matching IC units relative to KS\n",
    "match_to_ks = match_to_ks.rename('match')\n",
    "match_filename = results_path/'si_match.csv'\n",
    "match_to_ks.to_csv(match_filename) # header=False\n",
    "\n",
    "\n",
    "# # Save all the best agreement scores for each KS cell (not necessarily the matched scores)\n",
    "# # score_df = pd.DataFrame({'ks_ind': range(len(match_to_ks)), 'ic_ind':match_to_ks,'score': comp_ks_ic.agreement_scores.max(axis=1) })\n",
    "# score_filename = results_path/'si_score_all.csv'\n",
    "# comp_ks_ic.agreement_scores.to_csv(score_filename)\n",
    "\n",
    "# Example of loading the curated phy output back in and selecting units marked \"good\"\n",
    "# sorting_phy = si.PhySortingExtractor('path-to-phy-folder', exclude_cluster_groups=['noise'])\n",
    "# good_ks_units = []\n",
    "# for u in sorting_phy.get_unit_ids():\n",
    "#     if sorting_phy.get_unit_property(u, 'KSLabel') == 'good':\n",
    "#         good_ks_units.append(u)        \n",
    "# sorting_ks_good = sorting_phy.select_units(good_ks_units)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export final comparison sorting to Phy?\n",
    "(Not necessary if I label good cells in existing kilosort folder - but save any manual sorting first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "data_path = [f for f in results_path.iterdir() if f.is_dir() and f.name.startswith('raw')] # subfolder starting with raw*. Should only be one!\n",
    "data_path = data_path[0] # Set data_path to the folder containing the intan.rhd file etc\n",
    "print(data_path)\n",
    "recording_raw = spikeinterface_hp.read_intan_dat(data_path)\n",
    "\n",
    "# Load the probe for this file - should automatically deal with missing channels!\n",
    "probe = load_probe_rhd.H6(data_path/'info.rhd', results_path/'probe.json')\n",
    "recording_raw = recording_raw.set_probe(probe, group_mode=\"by_shank\")\n",
    "\n",
    "# Filter\n",
    "recording_f = si.highpass_filter(recording_raw, freq_min=300) # Not recommended to throw out high frequencies before sortin\n",
    "recording_cmr = si.common_reference(recording_f, reference='global', operator='median')\n",
    "\n",
    "To export the original sortings to phy:\n",
    "\n",
    "# Get waveforms\n",
    "sub = '' #add onto output folders\n",
    "waveform_folder = results_path/('clean_waveforms'+sub)\n",
    "we = si.extract_waveforms(recording_cmr, sorting_clean, waveform_folder,return_scaled=True, overwrite=True,\n",
    "    n_jobs=n_jobs, chunk_duration=chunk_duration, max_spikes_per_unit=max_spikes_per_unit,ms_before=ms_before, ms_after=ms_after)\n",
    "\n",
    "\n",
    "chunk_duration = '1s'\n",
    "max_ch_per_template = 1\n",
    "\n",
    "# Export to Phy\n",
    "phy_folder = results_path/('clean_phy'+sub)\n",
    "we.recording = recording_raw # Just do this so that the path to the raw .dat file is stored in params.py\n",
    "si.export_to_phy(we, output_folder=phy_folder,remove_if_exists=True,copy_binary=False,compute_amplitudes=True,\n",
    "    max_channels_per_template=max_ch_per_template, chunk_duration=chunk_duration, n_jobs=n_jobs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Get Quality metrics for KS sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract waveforms\n",
    "waveform_folder = results_path/('kilosort2_waveforms')\n",
    "print(waveform_folder)\n",
    "we = si.extract_waveforms(recording_cmr, sorting_ks, waveform_folder, return_scaled=True, overwrite=True, # TODO check effect of raw or cmr??\n",
    "    n_jobs=n_jobs, chunk_duration=chunk_duration, max_spikes_per_unit=max_spikes_per_unit,ms_before=ms_before, ms_after=ms_after) #total_memory=\"10M\",  \n",
    "    #NOTE: return_scaled: If True and recording has gain_to_uV/offset_to_uV properties, waveforms are converted to uV.\n",
    "\n",
    "\n",
    "# Get quality metrics for all KS sorting\n",
    "waveform_folder = results_path/'kilosort2_waveforms'\n",
    "we = si.WaveformExtractor.load_from_folder(waveform_folder) # Load the waveforms results (only needed if picking up from here)\n",
    "metric_names=['snr', 'isi_violation', 'amplitude_cutoff','isolation_distance','firing_rate']\n",
    "# NOTE: isi_violations_rate: \"Rate of contaminating spikes as a fraction of overall rate. Higher values indicate more contamination\" (== #sp_bad/#sp_total)\n",
    "# Q: how to set bin width and refractory period for ISI violations?\n",
    "\n",
    "qm = si.compute_quality_metrics(we,load_if_exists=True,metric_names=metric_names)\n",
    "\n",
    "score_filename = results_path/'si_qm.csv'\n",
    "qm.to_csv(score_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Quality metrics & automatic curation for single sorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "waveform_folder = results_path/'kilosort2_waveforms'\n",
    "\n",
    "# Load waveforms\n",
    "we = si.WaveformExtractor.load_from_folder(waveform_folder) # Only needed if picking up from here\n",
    "\n",
    "# Specifiy quality metrics\n",
    "print(si.get_quality_metric_list())\n",
    "metric_names=['snr', 'isi_violation', 'amplitude_cutoff','isolation_distance','firing_rate']\n",
    "# NOTE: isi_violations_rate: \"Rate of contaminating spikes as a fraction of overall rate. Higher values indicate more contamination\" (== #sp_bad/#sp_total)\n",
    "# Q: how to set bin width and refractory period for ISI violations?\n",
    "\n",
    "# Compute quality metrics\n",
    "qm = si.compute_quality_metrics(we,load_if_exists=True,metric_names=metric_names)\n",
    "pprint(qm)\n",
    "\n",
    "# Plot quality metrics\n",
    "si.plot_quality_metrics(we, include_metrics=[\"amplitude_cutoff\", \"isi_violations_rate\",\"firing_rate\"])\n",
    "\n",
    "# Screen sorting based on quality metrics\n",
    "firing_rate_cutoff = 50\n",
    "isi_viol_thresh =  0.1 # 0.2 is KS2 default\n",
    "amplitude_cutoff_thresh = 0.05\n",
    "our_query = f\"firing_rate < {firing_rate_cutoff} & isi_violations_rate < {isi_viol_thresh} & amplitude_cutoff < {amplitude_cutoff_thresh} \"\n",
    "\n",
    "qm_keep = qm.query(our_query)\n",
    "keep_unit_ids = qm_keep.index.values\n",
    "sorting_clean = we.sorting.select_units(keep_unit_ids)\n",
    "print(sorting)\n",
    "print(sorting_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Launch phy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%script false # don't run this cell!\n",
    "\n",
    "# Launch Phy\n",
    "phy_folder = results_path/'clean_phy'\n",
    "from phy.apps.template import template_gui\n",
    "template_gui(phy_folder/'params.py')\n",
    "Parking lot\n",
    "%%script false # don't run this cell!\n",
    "\n",
    "# Check data type of .npy file\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "# file = Path(r\"D:\\hannah\\Dropbox\\code\\spikesort\\spikesort-hp\\results\\kilosort2_waveforms\\spike_amplitudes\\amplitude_segment_0.npy\")\n",
    "file = Path(r\"Z:\\Hannah\\ephys\\project2\\HC05_220819\\intersect_waveforms\\spike_amplitudes\\amplitude_segment_0.npy\")\n",
    "print(file)\n",
    "temp = np.load(file)\n",
    "print(type(temp))\n",
    "print(type(temp[0]))\n",
    "pprint(temp)\n",
    "np.shape(temp)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:50:36) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e87c389955eb3b5496154ae0f9a13093fb0f1de7e15261958ba371a00cba5d1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
